{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Lambda Learning Rate"
      ],
      "metadata": {
        "id": "yNb2iiUbAFgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qJjb9XuIpleg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb699d7-c8fa-44ad-950c-c5aab9fb1a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.0, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
            "0.010000000000000002\n",
            "0.020000000000000004\n",
            "0.03\n",
            "0.04000000000000001\n",
            "0.05\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "lr = 0.1\n",
        "model = nn.Linear(10,1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "lambda1 = lambda epoch: epoch / 10\n",
        "\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lambda1)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "for epoch in range(5):\n",
        "    #loss.backward()\n",
        "    optimizer.step()\n",
        "    # validate (...)\n",
        "    scheduler.step()\n",
        "    print(optimizer.state_dict()['param_groups'][0]['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplicative Learning Rate"
      ],
      "metadata": {
        "id": "NSjpy67TAMT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "lr = 0.1\n",
        "model = nn.Linear(10,1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "lambda1 = lambda epoch: 0.95\n",
        "\n",
        "scheduler = lr_scheduler.MultiplicativeLR(optimizer, lambda1)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "for epoch in range(5):\n",
        "    #loss.backward()\n",
        "    optimizer.step()\n",
        "    # validate (...)\n",
        "    scheduler.step()\n",
        "    print(optimizer.state_dict()['param_groups'][0]['lr'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x5CoCAe_XUl",
        "outputId": "55bfc151-d6a6-4559-a775-b0e71e25e4bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
            "0.095\n",
            "0.09025\n",
            "0.0857375\n",
            "0.08145062499999998\n",
            "0.07737809374999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step Learning Rate"
      ],
      "metadata": {
        "id": "V0AufDuN__rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming optimizer uses lr = 0.05 for all groups\n",
        "# lr = 0.05 if epoch < 30\n",
        "# lr = 0.005 if 30 <= epoch < 60\n",
        "# lr = 0.0005 if 60 <= epoch < 90\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "lr = 0.1\n",
        "model = nn.Linear(10,1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "for epoch in range(5):\n",
        "    #loss.backward()\n",
        "    optimizer.step()\n",
        "    # validate (...)\n",
        "    scheduler.step()\n",
        "    print(optimizer.state_dict()['param_groups'][0]['lr'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1FCaQPsAwve",
        "outputId": "a38d1c6a-defa-447c-85e7-157ab56c8c63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'initial_lr': 0.1, 'params': [0, 1]}]}\n",
            "0.1\n",
            "0.1\n",
            "0.010000000000000002\n",
            "0.010000000000000002\n",
            "0.010000000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ImwSTcbCSJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}